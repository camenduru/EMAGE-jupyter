{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/EMAGE-jupyter/blob/main/test.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone -b dev https://github.com/camenduru/EMAGE-hf\n",
        "%cd /content/EMAGE-hf\n",
        "\n",
        "!pip install -q loguru==0.7.2 smplx ConfigArgParse==1.7 trimesh==3.23.5 lmdb==1.4.1 fasttext-wheel textgrid==1.5 pyvirtualdisplay==3.0 pyrender\n",
        "\n",
        "!apt -y install -qq aria2\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/H-Liu1997/EMAGE/resolve/main/EMAGE/emage_audio_175.bin -d /content/EMAGE-hf/EMAGE -o emage_audio_175.bin\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/H-Liu1997/EMAGE/resolve/main/EMAGE/pretrained_vq/hands_vertex_1layer_710.bin -d /content/EMAGE-hf/EMAGE/pretrained_vq -o hands_vertex_1layer_710.bin\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/H-Liu1997/EMAGE/resolve/main/EMAGE/pretrained_vq/last_1700_foot.bin -d /content/EMAGE-hf/EMAGE/pretrained_vq -o last_1700_foot.bin\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/H-Liu1997/EMAGE/resolve/main/EMAGE/pretrained_vq/last_790_face_v2.bin -d /content/EMAGE-hf/EMAGE/pretrained_vq -o last_790_face_v2.bin\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/H-Liu1997/EMAGE/resolve/main/EMAGE/pretrained_vq/lower_foot_600.bin -d /content/EMAGE-hf/EMAGE/pretrained_vq -o lower_foot_600.bin\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/H-Liu1997/EMAGE/resolve/main/EMAGE/pretrained_vq/upper_vertex_1layer_710.bin -d /content/EMAGE-hf/EMAGE/pretrained_vq -o upper_vertex_1layer_710.bin\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/H-Liu1997/EMAGE/resolve/main/EMAGE/smplx_models/smplx/SMPLX_NEUTRAL_2020.npz -d /content/EMAGE-hf/EMAGE/smplx_models/smplx -o SMPLX_NEUTRAL_2020.npz\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/H-Liu1997/EMAGE/resolve/main/EMAGE/test_sequences/smplxflame_30/2_scott_0_1_1.npz -d /content/EMAGE-hf/EMAGE/test_sequences/smplxflame_30 -o 2_scott_0_1_1.npz\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/H-Liu1997/EMAGE/resolve/main/EMAGE/test_sequences/smplxflame_30/2_scott_0_2_2.npz -d /content/EMAGE-hf/EMAGE/test_sequences/smplxflame_30 -o 2_scott_0_2_2.npz\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/H-Liu1997/EMAGE/resolve/main/EMAGE/test_sequences/smplxflame_30/2_scott_0_3_3.npz -d /content/EMAGE-hf/EMAGE/test_sequences/smplxflame_30 -o 2_scott_0_3_3.npz\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/H-Liu1997/EMAGE/resolve/main/EMAGE/test_sequences/smplxflame_30/2_scott_0_4_4.npz -d /content/EMAGE-hf/EMAGE/test_sequences/smplxflame_30 -o 2_scott_0_4_4.npz\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/H-Liu1997/EMAGE/resolve/main/EMAGE/test_sequences/wave16k/2_scott_0_1_1.wav -d /content/EMAGE-hf/EMAGE/test_sequences/wave16k -o 2_scott_0_1_1.wav\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/H-Liu1997/EMAGE/resolve/main/EMAGE/test_sequences/wave16k/2_scott_0_2_2.wav -d /content/EMAGE-hf/EMAGE/test_sequences/wave16k -o 2_scott_0_2_2.wav\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/H-Liu1997/EMAGE/resolve/main/EMAGE/test_sequences/wave16k/2_scott_0_3_3.wav -d /content/EMAGE-hf/EMAGE/test_sequences/wave16k -o 2_scott_0_3_3.wav\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/H-Liu1997/EMAGE/resolve/main/EMAGE/test_sequences/wave16k/2_scott_0_4_4.wav -d /content/EMAGE-hf/EMAGE/test_sequences/wave16k -o 2_scott_0_4_4.wav\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/H-Liu1997/EMAGE/resolve/main/EMAGE/test_sequences/weights/AESKConv_240_100.bin -d /content/EMAGE-hf/EMAGE/test_sequences/weights -o AESKConv_240_100.bin\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/H-Liu1997/EMAGE/resolve/main/EMAGE/test_sequences/weights/mean_vel_smplxflame_30.npy -d /content/EMAGE-hf/EMAGE/test_sequences/weights -o mean_vel_smplxflame_30.npy\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/H-Liu1997/EMAGE/resolve/main/EMAGE/test_sequences/weights/vocab.pkl -d /content/EMAGE-hf/EMAGE/test_sequences/weights -o vocab.pkl\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/H-Liu1997/EMAGE/resolve/main/outputs/audio2pose/custom/hf/999/gt_2_scott_0_3_3.npz -d /content/EMAGE-hf/outputs/audio2pose/custom/hf/999 -o gt_2_scott_0_3_3.npz\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/H-Liu1997/EMAGE/resolve/main/outputs/audio2pose/custom/hf/999/res_2_scott_0_3_3.npz -d /content/EMAGE-hf/outputs/audio2pose/custom/hf/999 -o res_2_scott_0_3_3.npz\n",
        "\n",
        "class Options:\n",
        "    def __init__(self):\n",
        "        self.a_encoder = None\n",
        "        self.a_fix_pre = False\n",
        "        self.a_pre_encoder = None\n",
        "        self.acc_weight = 0.0\n",
        "        self.additional_data = False\n",
        "        self.adv_weight = 20.0\n",
        "        self.ali_weight = 0.0\n",
        "        self.amsgrad = False\n",
        "        self.apex = False\n",
        "        self.asmr = 0.0\n",
        "        self.atcont = 0.0\n",
        "        self.atmr = 0.0\n",
        "        self.aud_prob = 1.0\n",
        "        self.audio_dims = 1\n",
        "        self.audio_f = 256\n",
        "        self.audio_fps = 16000\n",
        "        self.audio_norm = False\n",
        "        self.audio_rep = 'wave16k'\n",
        "        self.audio_sr = 16000\n",
        "        self.batch_size = 64\n",
        "        self.beat_align = True\n",
        "        self.benchmark = True\n",
        "        self.cache_only = False\n",
        "        self.cache_path = './datasets/beat_cache/beat_smplx_en_emage_test/'\n",
        "        self.cf = 0.0\n",
        "        self.ch = 1.0\n",
        "        self.cl = 1.0\n",
        "        self.clean_final_seconds = 0\n",
        "        self.clean_first_seconds = 0\n",
        "        self.config = './configs/emage_test_hf.yaml'\n",
        "        self.csv_name = 'a2g_0'\n",
        "        self.cu = 1.0\n",
        "        self.cudnn_enabled = True\n",
        "        self.d_lr_weight = 0.2\n",
        "        self.d_name = None\n",
        "        self.data_path = './EMAGE/test_sequences/'\n",
        "        self.data_path_1 = './EMAGE/'\n",
        "        self.dataset = 'beat_testonly_hf'\n",
        "        self.ddp = False\n",
        "        self.debug = False\n",
        "        self.decay_epochs = 9999\n",
        "        self.decay_rate = 0.1\n",
        "        self.decode_fusion = None\n",
        "        self.deterministic = True\n",
        "        self.disable_filtering = False\n",
        "        self.div_reg_weight = 0.0\n",
        "        self.dropout_prob = 0.3\n",
        "        self.e_name = 'VAESKConv'\n",
        "        self.e_path = 'weights/AESKConv_240_100.bin'\n",
        "        self.emo_rep = None\n",
        "        self.emotion_dims = 8\n",
        "        self.emotion_f = 0\n",
        "        self.epoch_stage = 0\n",
        "        self.epochs = 400\n",
        "        self.eval_model = 'motion_representation'\n",
        "        self.f_encoder = 'null'\n",
        "        self.f_fix_pre = False\n",
        "        self.f_pre_encoder = 'null'\n",
        "        self.fac_prob = 1.0\n",
        "        self.facial_dims = 100\n",
        "        self.facial_f = 0\n",
        "        self.facial_fps = 15\n",
        "        self.facial_norm = False\n",
        "        self.facial_rep = 'smplxflame_30'\n",
        "        self.fid_weight = 0.0\n",
        "        self.finger_net = 'original'\n",
        "        self.freeze_wordembed = True\n",
        "        self.fsmr = 0.0\n",
        "        self.ftmr = 0.0\n",
        "        self.fusion_mode = 'sum'\n",
        "        self.g_name = 'MAGE_Transformer'\n",
        "        self.gap_weight = 0.0\n",
        "        self.gpus = [0]\n",
        "        self.grad_norm = 0.99\n",
        "        self.hidden_size = 768\n",
        "        self.id_rep = 'onehot'\n",
        "        self.input_context = 'both'\n",
        "        self.is_train = True\n",
        "        self.ita_weight = 0.0\n",
        "        self.iwa_weight = 0.0\n",
        "        self.kld_aud_weight = 0.0\n",
        "        self.kld_fac_weight = 0.0\n",
        "        self.kld_weight = 0.0\n",
        "        self.l = 4\n",
        "        self.lf = 3.0\n",
        "        self.lh = 3.0\n",
        "        self.ll = 3.0\n",
        "        self.loader_workers = 0\n",
        "        self.log_period = 10\n",
        "        self.loss_contrastive_neg_weight = 0.005\n",
        "        self.loss_contrastive_pos_weight = 0.2\n",
        "        self.loss_gan_weight = 5.0\n",
        "        self.loss_kld_weight = 0.1\n",
        "        self.loss_physical_weight = 0.0\n",
        "        self.loss_reg_weight = 0.05\n",
        "        self.loss_regression_weight = 70.0\n",
        "        self.lr_base = 0.0005\n",
        "        self.lr_min = 1e-07\n",
        "        self.lr_policy = 'step'\n",
        "        self.lu = 3.0\n",
        "        self.m_decoder = None\n",
        "        self.m_encoder = 'null'\n",
        "        self.m_fix_pre = False\n",
        "        self.m_pre_encoder = 'null'\n",
        "        self.mean_pose_path = '/datasets/trinity/train/'\n",
        "        self.model = 'emage_audio'\n",
        "        self.momentum = 0.8\n",
        "        self.motion_f = 256\n",
        "        self.msmr = 0.0\n",
        "        self.mtmr = 0.0\n",
        "        self.multi_length_training = [1.0]\n",
        "        self.n_layer = 1\n",
        "        self.n_poses = 34\n",
        "        self.n_pre_poses = 4\n",
        "        self.name = '0409_042545_emage_test_hf'\n",
        "        self.nesterov = True\n",
        "        self.new_cache = True\n",
        "        self.no_adv_epoch = 999\n",
        "        self.notes = ''\n",
        "        self.opt = 'adam'\n",
        "        self.opt_betas = [0.5, 0.999]\n",
        "        self.ori_joints = 'beat_smplx_joints'\n",
        "        self.out_path = './outputs/audio2pose/'\n",
        "        self.pos_encoding_type = 'sin'\n",
        "        self.pos_prob = 1.0\n",
        "        self.pose_dims = 330\n",
        "        self.pose_fps = 30\n",
        "        self.pose_length = 64\n",
        "        self.pose_norm = False\n",
        "        self.pose_rep = 'smplxflame_30'\n",
        "        self.pre_frames = 4\n",
        "        self.pre_type = 'zero'\n",
        "        self.pretrain = False\n",
        "        self.project = 's2g'\n",
        "        self.queue_size = 1024\n",
        "        self.random_seed = 2021\n",
        "        self.rec_aud_weight = 0.0\n",
        "        self.rec_fac_weight = 0.0\n",
        "        self.rec_pos_weight = 0.0\n",
        "        self.rec_txt_weight = 0.0\n",
        "        self.rec_ver_weight = 0.0\n",
        "        self.rec_weight = 1.0\n",
        "        self.render_concurrent_num = 1\n",
        "        self.render_tmp_img_filetype = 'bmp'\n",
        "        self.render_video_fps = 30\n",
        "        self.render_video_height = 720\n",
        "        self.render_video_width = 1920\n",
        "        self.root_path = './'\n",
        "        self.rot6d = True\n",
        "        self.sem_rep = None\n",
        "        self.sparse = 1\n",
        "        self.speaker_dims = 4\n",
        "        self.speaker_f = 0\n",
        "        self.speaker_id = 'onehot'\n",
        "        self.stat = 'ts'\n",
        "        self.std_pose_path = '/datasets/trinity/train/'\n",
        "        self.stride = 20\n",
        "        self.t_encoder = None\n",
        "        self.t_fix_pre = False\n",
        "        self.t_pre_encoder = None\n",
        "        self.tar_joints = 'beat_smplx_full'\n",
        "        self.test_ckpt = './EMAGE/emage_audio_175.bin'\n",
        "        self.test_data_path = '/datasets/trinity/test/'\n",
        "        self.test_length = 64\n",
        "        self.test_period = 20\n",
        "        self.train_data_path = '/datasets/trinity/train/'\n",
        "        self.train_trans = True\n",
        "        self.trainer = 'emage'\n",
        "        self.training_speakers = [2]\n",
        "        self.tsmr = 0.0\n",
        "        self.ttmr = 0.0\n",
        "        self.txt_prob = 1.0\n",
        "        self.use_aug = False\n",
        "        self.vae_codebook_size = 256\n",
        "        self.vae_grow = [1, 1, 2, 1]\n",
        "        self.vae_layer = 4\n",
        "        self.vae_length = 240\n",
        "        self.vae_quantizer_lambda = 1.0\n",
        "        self.vae_test_dim = 330\n",
        "        self.vae_test_len = 32\n",
        "        self.vae_test_stride = 20\n",
        "        self.val_data_path = '/datasets/trinity/val/'\n",
        "        self.variational = False\n",
        "        self.vel_weight = 0.0\n",
        "        self.warmup_epochs = 0\n",
        "        self.warmup_lr = 0.0005\n",
        "        self.wei_weight = 0.0\n",
        "        self.weight_decay = 0.0\n",
        "        self.word_cache = False\n",
        "        self.word_dims = 300\n",
        "        self.word_f = 0\n",
        "        self.word_index_num = 5793\n",
        "        self.word_rep = None\n",
        "        self.z_type = 'speaker'\n",
        "\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "import warnings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import numpy as np\n",
        "import time\n",
        "from loguru import logger\n",
        "import smplx\n",
        "from utils import other_tools_hf\n",
        "from dataloaders.data_tools import joints_list\n",
        "from utils import rotation_conversions as rc\n",
        "import soundfile as sf\n",
        "import librosa \n",
        "\n",
        "def inverse_selection_tensor(filtered_t, selection_array, n):\n",
        "    selection_array = torch.from_numpy(selection_array).cuda()\n",
        "    original_shape_t = torch.zeros((n, 165)).cuda()\n",
        "    selected_indices = torch.where(selection_array == 1)[0]\n",
        "    for i in range(n):\n",
        "        original_shape_t[i, selected_indices] = filtered_t[i]\n",
        "    return original_shape_t\n",
        "\n",
        "def test_demo_gpu(\n",
        "    model, vq_model_face, vq_model_upper, vq_model_hands, vq_model_lower, global_motion, smplx_model,\n",
        "    dict_data,\n",
        "    args,\n",
        "    joints, joint_mask_upper, joint_mask_lower, joint_mask_hands,\n",
        "    log_softmax,\n",
        "):\n",
        "    rank = 0\n",
        "    other_tools_hf.load_checkpoints(vq_model_face, args.data_path_1 + \"pretrained_vq/last_790_face_v2.bin\", args.e_name)\n",
        "    other_tools_hf.load_checkpoints(vq_model_upper, args.data_path_1 + \"pretrained_vq/upper_vertex_1layer_710.bin\", args.e_name)\n",
        "    other_tools_hf.load_checkpoints(vq_model_hands, args.data_path_1 + \"pretrained_vq/hands_vertex_1layer_710.bin\", args.e_name)\n",
        "    other_tools_hf.load_checkpoints(vq_model_lower, args.data_path_1 + \"pretrained_vq/lower_foot_600.bin\", args.e_name)\n",
        "    other_tools_hf.load_checkpoints(global_motion, args.data_path_1 + \"pretrained_vq/last_1700_foot.bin\", args.e_name)\n",
        "    other_tools_hf.load_checkpoints(model, args.test_ckpt, args.g_name)\n",
        "    model.to(rank).eval()\n",
        "    smplx_model.to(rank).eval()\n",
        "    vq_model_face.to(rank).eval()\n",
        "    vq_model_upper.to(rank).eval()\n",
        "    vq_model_hands.to(rank).eval()\n",
        "    vq_model_lower.to(rank).eval()\n",
        "    global_motion.to(rank).eval()\n",
        "        \n",
        "    with torch.no_grad():\n",
        "        tar_pose_raw = dict_data[\"pose\"]\n",
        "        tar_pose = tar_pose_raw[:, :, :165].to(rank)\n",
        "        tar_contact = tar_pose_raw[:, :, 165:169].to(rank)\n",
        "        tar_trans = dict_data[\"trans\"].to(rank)\n",
        "        tar_exps = dict_data[\"facial\"].to(rank)\n",
        "        in_audio = dict_data[\"audio\"].to(rank) \n",
        "        in_word = None# dict_data[\"word\"].to(rank)\n",
        "        tar_beta = dict_data[\"beta\"].to(rank)\n",
        "        tar_id = dict_data[\"id\"].to(rank).long()\n",
        "        bs, n, j = tar_pose.shape[0], tar_pose.shape[1], joints\n",
        "\n",
        "        tar_pose_jaw = tar_pose[:, :, 66:69]\n",
        "        tar_pose_jaw = rc.axis_angle_to_matrix(tar_pose_jaw.reshape(bs, n, 1, 3))\n",
        "        tar_pose_jaw = rc.matrix_to_rotation_6d(tar_pose_jaw).reshape(bs, n, 1*6)\n",
        "        tar_pose_face = torch.cat([tar_pose_jaw, tar_exps], dim=2)\n",
        "\n",
        "        tar_pose_hands = tar_pose[:, :, 25*3:55*3]\n",
        "        tar_pose_hands = rc.axis_angle_to_matrix(tar_pose_hands.reshape(bs, n, 30, 3))\n",
        "        tar_pose_hands = rc.matrix_to_rotation_6d(tar_pose_hands).reshape(bs, n, 30*6)\n",
        "\n",
        "        tar_pose_upper = tar_pose[:, :, joint_mask_upper.astype(bool)]\n",
        "        tar_pose_upper = rc.axis_angle_to_matrix(tar_pose_upper.reshape(bs, n, 13, 3))\n",
        "        tar_pose_upper = rc.matrix_to_rotation_6d(tar_pose_upper).reshape(bs, n, 13*6)\n",
        "\n",
        "        tar_pose_leg = tar_pose[:, :, joint_mask_lower.astype(bool)]\n",
        "        tar_pose_leg = rc.axis_angle_to_matrix(tar_pose_leg.reshape(bs, n, 9, 3))\n",
        "        tar_pose_leg = rc.matrix_to_rotation_6d(tar_pose_leg).reshape(bs, n, 9*6)\n",
        "        tar_pose_lower = torch.cat([tar_pose_leg, tar_trans, tar_contact], dim=2)\n",
        "\n",
        "        # tar_pose = rc.axis_angle_to_matrix(tar_pose.reshape(bs, n, j, 3))\n",
        "        # tar_pose = rc.matrix_to_rotation_6d(tar_pose).reshape(bs, n, j*6)\n",
        "        tar4dis = torch.cat([tar_pose_jaw, tar_pose_upper, tar_pose_hands, tar_pose_leg], dim=2)\n",
        "\n",
        "        tar_index_value_face_top = vq_model_face.map2index(tar_pose_face) # bs*n/4\n",
        "        tar_index_value_upper_top = vq_model_upper.map2index(tar_pose_upper) # bs*n/4\n",
        "        tar_index_value_hands_top = vq_model_hands.map2index(tar_pose_hands) # bs*n/4\n",
        "        tar_index_value_lower_top = vq_model_lower.map2index(tar_pose_lower) # bs*n/4\n",
        "      \n",
        "        latent_face_top = vq_model_face.map2latent(tar_pose_face) # bs*n/4\n",
        "        latent_upper_top = vq_model_upper.map2latent(tar_pose_upper) # bs*n/4\n",
        "        latent_hands_top = vq_model_hands.map2latent(tar_pose_hands) # bs*n/4\n",
        "        latent_lower_top = vq_model_lower.map2latent(tar_pose_lower) # bs*n/4\n",
        "        \n",
        "        latent_in = torch.cat([latent_upper_top, latent_hands_top, latent_lower_top], dim=2)\n",
        "        \n",
        "        index_in = torch.stack([tar_index_value_upper_top, tar_index_value_hands_top, tar_index_value_lower_top], dim=-1).long()\n",
        "        \n",
        "        tar_pose_6d = rc.axis_angle_to_matrix(tar_pose.reshape(bs, n, 55, 3))\n",
        "        tar_pose_6d = rc.matrix_to_rotation_6d(tar_pose_6d).reshape(bs, n, 55*6)\n",
        "        latent_all = torch.cat([tar_pose_6d, tar_trans, tar_contact], dim=-1)\n",
        "\n",
        "        loaded_data = {\n",
        "            \"tar_pose_jaw\": tar_pose_jaw,\n",
        "            \"tar_pose_face\": tar_pose_face,\n",
        "            \"tar_pose_upper\": tar_pose_upper,\n",
        "            \"tar_pose_lower\": tar_pose_lower,\n",
        "            \"tar_pose_hands\": tar_pose_hands,\n",
        "            'tar_pose_leg': tar_pose_leg,\n",
        "            \"in_audio\": in_audio,\n",
        "            \"in_word\": in_word,\n",
        "            \"tar_trans\": tar_trans,\n",
        "            \"tar_exps\": tar_exps,\n",
        "            \"tar_beta\": tar_beta,\n",
        "            \"tar_pose\": tar_pose,\n",
        "            \"tar4dis\": tar4dis,\n",
        "            \"tar_index_value_face_top\": tar_index_value_face_top,\n",
        "            \"tar_index_value_upper_top\": tar_index_value_upper_top,\n",
        "            \"tar_index_value_hands_top\": tar_index_value_hands_top,\n",
        "            \"tar_index_value_lower_top\": tar_index_value_lower_top,\n",
        "            \"latent_face_top\": latent_face_top,\n",
        "            \"latent_upper_top\": latent_upper_top,\n",
        "            \"latent_hands_top\": latent_hands_top,\n",
        "            \"latent_lower_top\": latent_lower_top,\n",
        "            \"latent_in\":  latent_in,\n",
        "            \"index_in\": index_in,\n",
        "            \"tar_id\": tar_id,\n",
        "            \"latent_all\": latent_all,\n",
        "            \"tar_pose_6d\": tar_pose_6d,\n",
        "            \"tar_contact\": tar_contact,\n",
        "        }\n",
        "\n",
        "        mode = 'test'\n",
        "        bs, n, j = loaded_data[\"tar_pose\"].shape[0], loaded_data[\"tar_pose\"].shape[1], joints \n",
        "        tar_pose = loaded_data[\"tar_pose\"]\n",
        "        tar_beta = loaded_data[\"tar_beta\"]\n",
        "        in_word =None# loaded_data[\"in_word\"]\n",
        "        tar_exps = loaded_data[\"tar_exps\"]\n",
        "        tar_contact = loaded_data[\"tar_contact\"]\n",
        "        in_audio = loaded_data[\"in_audio\"]\n",
        "        tar_trans = loaded_data[\"tar_trans\"]\n",
        "      \n",
        "        remain = n%8\n",
        "        if remain != 0:\n",
        "            tar_pose = tar_pose[:, :-remain, :]\n",
        "            tar_beta = tar_beta[:, :-remain, :]\n",
        "            tar_trans = tar_trans[:, :-remain, :]\n",
        "            # in_word = in_word[:, :-remain]\n",
        "            tar_exps = tar_exps[:, :-remain, :]\n",
        "            tar_contact = tar_contact[:, :-remain, :]\n",
        "            n = n - remain\n",
        "\n",
        "        tar_pose_jaw = tar_pose[:, :, 66:69]\n",
        "        tar_pose_jaw = rc.axis_angle_to_matrix(tar_pose_jaw.reshape(bs, n, 1, 3))\n",
        "        tar_pose_jaw = rc.matrix_to_rotation_6d(tar_pose_jaw).reshape(bs, n, 1*6)\n",
        "        tar_pose_face = torch.cat([tar_pose_jaw, tar_exps], dim=2)\n",
        "\n",
        "        tar_pose_hands = tar_pose[:, :, 25*3:55*3]\n",
        "        tar_pose_hands = rc.axis_angle_to_matrix(tar_pose_hands.reshape(bs, n, 30, 3))\n",
        "        tar_pose_hands = rc.matrix_to_rotation_6d(tar_pose_hands).reshape(bs, n, 30*6)\n",
        "\n",
        "        tar_pose_upper = tar_pose[:, :, joint_mask_upper.astype(bool)]\n",
        "        tar_pose_upper = rc.axis_angle_to_matrix(tar_pose_upper.reshape(bs, n, 13, 3))\n",
        "        tar_pose_upper = rc.matrix_to_rotation_6d(tar_pose_upper).reshape(bs, n, 13*6)\n",
        "\n",
        "        tar_pose_leg = tar_pose[:, :, joint_mask_lower.astype(bool)]\n",
        "        tar_pose_leg = rc.axis_angle_to_matrix(tar_pose_leg.reshape(bs, n, 9, 3))\n",
        "        tar_pose_leg = rc.matrix_to_rotation_6d(tar_pose_leg).reshape(bs, n, 9*6)\n",
        "        tar_pose_lower = torch.cat([tar_pose_leg, tar_trans, tar_contact], dim=2)\n",
        "        \n",
        "        tar_pose_6d = rc.axis_angle_to_matrix(tar_pose.reshape(bs, n, 55, 3))\n",
        "        tar_pose_6d = rc.matrix_to_rotation_6d(tar_pose_6d).reshape(bs, n, 55*6)\n",
        "        latent_all = torch.cat([tar_pose_6d, tar_trans, tar_contact], dim=-1)\n",
        "        \n",
        "        rec_index_all_face = []\n",
        "        rec_index_all_upper = []\n",
        "        rec_index_all_lower = []\n",
        "        rec_index_all_hands = []\n",
        "        \n",
        "        roundt = (n - args.pre_frames) // (args.pose_length - args.pre_frames)\n",
        "        remain = (n - args.pre_frames) % (args.pose_length - args.pre_frames)\n",
        "        round_l = args.pose_length - args.pre_frames\n",
        "\n",
        "        for i in range(0, roundt):\n",
        "            # in_word_tmp = in_word[:, i*(round_l):(i+1)*(round_l)+args.pre_frames]\n",
        "            # audio fps is 16000 and pose fps is 30\n",
        "            in_audio_tmp = in_audio[:, i*(16000//30*round_l):(i+1)*(16000//30*round_l)+16000//30*args.pre_frames]\n",
        "            in_id_tmp = loaded_data['tar_id'][:, i*(round_l):(i+1)*(round_l)+args.pre_frames]\n",
        "            mask_val = torch.ones(bs, args.pose_length, args.pose_dims+3+4).float().cuda()\n",
        "            mask_val[:, :args.pre_frames, :] = 0.0\n",
        "            if i == 0:\n",
        "                latent_all_tmp = latent_all[:, i*(round_l):(i+1)*(round_l)+args.pre_frames, :]\n",
        "            else:\n",
        "                latent_all_tmp = latent_all[:, i*(round_l):(i+1)*(round_l)+args.pre_frames, :]\n",
        "                # print(latent_all_tmp.shape, latent_last.shape)\n",
        "                latent_all_tmp[:, :args.pre_frames, :] = latent_last[:, -args.pre_frames:, :]\n",
        "            \n",
        "            net_out_val = model(\n",
        "                in_audio = in_audio_tmp,\n",
        "                in_word=None, #in_word_tmp,\n",
        "                mask=mask_val,\n",
        "                in_motion = latent_all_tmp,\n",
        "                in_id = in_id_tmp,\n",
        "                use_attentions=True,)\n",
        "            \n",
        "            if args.cu != 0:\n",
        "                rec_index_upper = log_softmax(net_out_val[\"cls_upper\"]).reshape(-1, args.vae_codebook_size)\n",
        "                _, rec_index_upper = torch.max(rec_index_upper.reshape(-1, args.pose_length, args.vae_codebook_size), dim=2)\n",
        "                #rec_upper = vq_model_upper.decode(rec_index_upper)\n",
        "            else:\n",
        "                _, rec_index_upper, _, _ = vq_model_upper.quantizer(net_out_val[\"rec_upper\"])\n",
        "                #rec_upper = vq_model_upper.decoder(rec_index_upper)\n",
        "            if args.cl != 0:\n",
        "                rec_index_lower = log_softmax(net_out_val[\"cls_lower\"]).reshape(-1, args.vae_codebook_size)\n",
        "                _, rec_index_lower = torch.max(rec_index_lower.reshape(-1, args.pose_length, args.vae_codebook_size), dim=2)\n",
        "                #rec_lower = vq_model_lower.decode(rec_index_lower)\n",
        "            else:\n",
        "                _, rec_index_lower, _, _ = vq_model_lower.quantizer(net_out_val[\"rec_lower\"])\n",
        "                #rec_lower = vq_model_lower.decoder(rec_index_lower)\n",
        "            if args.ch != 0:\n",
        "                rec_index_hands = log_softmax(net_out_val[\"cls_hands\"]).reshape(-1, args.vae_codebook_size)\n",
        "                _, rec_index_hands = torch.max(rec_index_hands.reshape(-1, args.pose_length, args.vae_codebook_size), dim=2)\n",
        "                #rec_hands = vq_model_hands.decode(rec_index_hands)\n",
        "            else:\n",
        "                _, rec_index_hands, _, _ = vq_model_hands.quantizer(net_out_val[\"rec_hands\"])\n",
        "                #rec_hands = vq_model_hands.decoder(rec_index_hands)\n",
        "            if args.cf != 0:\n",
        "                rec_index_face = log_softmax(net_out_val[\"cls_face\"]).reshape(-1, args.vae_codebook_size)\n",
        "                _, rec_index_face = torch.max(rec_index_face.reshape(-1, args.pose_length, args.vae_codebook_size), dim=2)\n",
        "                #rec_face = vq_model_face.decoder(rec_index_face)\n",
        "            else:\n",
        "                _, rec_index_face, _, _ = vq_model_face.quantizer(net_out_val[\"rec_face\"])\n",
        "                #rec_face = vq_model_face.decoder(rec_index_face)\n",
        "\n",
        "            if i == 0:\n",
        "                rec_index_all_face.append(rec_index_face)\n",
        "                rec_index_all_upper.append(rec_index_upper)\n",
        "                rec_index_all_lower.append(rec_index_lower)\n",
        "                rec_index_all_hands.append(rec_index_hands)\n",
        "            else:\n",
        "                rec_index_all_face.append(rec_index_face[:, args.pre_frames:])\n",
        "                rec_index_all_upper.append(rec_index_upper[:, args.pre_frames:])\n",
        "                rec_index_all_lower.append(rec_index_lower[:, args.pre_frames:])\n",
        "                rec_index_all_hands.append(rec_index_hands[:, args.pre_frames:])\n",
        "\n",
        "            if args.cu != 0:\n",
        "                rec_upper_last = vq_model_upper.decode(rec_index_upper)\n",
        "            else:\n",
        "                rec_upper_last = vq_model_upper.decoder(rec_index_upper)\n",
        "            if args.cl != 0:\n",
        "                rec_lower_last = vq_model_lower.decode(rec_index_lower)\n",
        "            else:\n",
        "                rec_lower_last = vq_model_lower.decoder(rec_index_lower)\n",
        "            if args.ch != 0:\n",
        "                rec_hands_last = vq_model_hands.decode(rec_index_hands)\n",
        "            else:\n",
        "                rec_hands_last = vq_model_hands.decoder(rec_index_hands)\n",
        "            # if args.cf != 0:\n",
        "            #     rec_face_last = vq_model_face.decode(rec_index_face)\n",
        "            # else:\n",
        "            #     rec_face_last = vq_model_face.decoder(rec_index_face)\n",
        "\n",
        "            rec_pose_legs = rec_lower_last[:, :, :54]\n",
        "            bs, n = rec_pose_legs.shape[0], rec_pose_legs.shape[1]\n",
        "            rec_pose_upper = rec_upper_last.reshape(bs, n, 13, 6)\n",
        "            rec_pose_upper = rc.rotation_6d_to_matrix(rec_pose_upper)#\n",
        "            rec_pose_upper = rc.matrix_to_axis_angle(rec_pose_upper).reshape(bs*n, 13*3)\n",
        "            rec_pose_upper_recover = inverse_selection_tensor(rec_pose_upper, joint_mask_upper, bs*n)\n",
        "            rec_pose_lower = rec_pose_legs.reshape(bs, n, 9, 6)\n",
        "            rec_pose_lower = rc.rotation_6d_to_matrix(rec_pose_lower)\n",
        "            rec_pose_lower = rc.matrix_to_axis_angle(rec_pose_lower).reshape(bs*n, 9*3)\n",
        "            rec_pose_lower_recover = inverse_selection_tensor(rec_pose_lower, joint_mask_lower, bs*n)\n",
        "            rec_pose_hands = rec_hands_last.reshape(bs, n, 30, 6)\n",
        "            rec_pose_hands = rc.rotation_6d_to_matrix(rec_pose_hands)\n",
        "            rec_pose_hands = rc.matrix_to_axis_angle(rec_pose_hands).reshape(bs*n, 30*3)\n",
        "            rec_pose_hands_recover = inverse_selection_tensor(rec_pose_hands, joint_mask_hands, bs*n)\n",
        "            rec_pose = rec_pose_upper_recover + rec_pose_lower_recover + rec_pose_hands_recover \n",
        "            rec_pose = rc.axis_angle_to_matrix(rec_pose.reshape(bs, n, j, 3))\n",
        "            rec_pose = rc.matrix_to_rotation_6d(rec_pose).reshape(bs, n, j*6)\n",
        "            rec_trans_v_s = rec_lower_last[:, :, 54:57]\n",
        "            rec_x_trans = other_tools_hf.velocity2position(rec_trans_v_s[:, :, 0:1], 1/args.pose_fps, tar_trans[:, 0, 0:1])\n",
        "            rec_z_trans = other_tools_hf.velocity2position(rec_trans_v_s[:, :, 2:3], 1/args.pose_fps, tar_trans[:, 0, 2:3])\n",
        "            rec_y_trans = rec_trans_v_s[:,:,1:2]\n",
        "            rec_trans = torch.cat([rec_x_trans, rec_y_trans, rec_z_trans], dim=-1)\n",
        "            latent_last = torch.cat([rec_pose, rec_trans, rec_lower_last[:, :, 57:61]], dim=-1)\n",
        "\n",
        "        rec_index_face = torch.cat(rec_index_all_face, dim=1)\n",
        "        rec_index_upper = torch.cat(rec_index_all_upper, dim=1)\n",
        "        rec_index_lower = torch.cat(rec_index_all_lower, dim=1)\n",
        "        rec_index_hands = torch.cat(rec_index_all_hands, dim=1)\n",
        "        if args.cu != 0:\n",
        "            rec_upper = vq_model_upper.decode(rec_index_upper)\n",
        "        else:\n",
        "            rec_upper = vq_model_upper.decoder(rec_index_upper)\n",
        "        if args.cl != 0:\n",
        "            rec_lower = vq_model_lower.decode(rec_index_lower)\n",
        "        else:\n",
        "            rec_lower = vq_model_lower.decoder(rec_index_lower)\n",
        "        if args.ch != 0:\n",
        "            rec_hands = vq_model_hands.decode(rec_index_hands)\n",
        "        else:\n",
        "            rec_hands = vq_model_hands.decoder(rec_index_hands)\n",
        "        if args.cf != 0:\n",
        "            rec_face = vq_model_face.decode(rec_index_face)\n",
        "        else:\n",
        "            rec_face = vq_model_face.decoder(rec_index_face)\n",
        "\n",
        "        rec_exps = rec_face[:, :, 6:]\n",
        "        rec_pose_jaw = rec_face[:, :, :6]\n",
        "        rec_pose_legs = rec_lower[:, :, :54]\n",
        "        bs, n = rec_pose_jaw.shape[0], rec_pose_jaw.shape[1]\n",
        "        rec_pose_upper = rec_upper.reshape(bs, n, 13, 6)\n",
        "        rec_pose_upper = rc.rotation_6d_to_matrix(rec_pose_upper)#\n",
        "        rec_pose_upper = rc.matrix_to_axis_angle(rec_pose_upper).reshape(bs*n, 13*3)\n",
        "        rec_pose_upper_recover = inverse_selection_tensor(rec_pose_upper, joint_mask_upper, bs*n)\n",
        "        rec_pose_lower = rec_pose_legs.reshape(bs, n, 9, 6)\n",
        "        rec_pose_lower = rc.rotation_6d_to_matrix(rec_pose_lower)\n",
        "        rec_lower2global = rc.matrix_to_rotation_6d(rec_pose_lower.clone()).reshape(bs, n, 9*6)\n",
        "        rec_pose_lower = rc.matrix_to_axis_angle(rec_pose_lower).reshape(bs*n, 9*3)\n",
        "        rec_pose_lower_recover = inverse_selection_tensor(rec_pose_lower, joint_mask_lower, bs*n)\n",
        "        rec_pose_hands = rec_hands.reshape(bs, n, 30, 6)\n",
        "        rec_pose_hands = rc.rotation_6d_to_matrix(rec_pose_hands)\n",
        "        rec_pose_hands = rc.matrix_to_axis_angle(rec_pose_hands).reshape(bs*n, 30*3)\n",
        "        rec_pose_hands_recover = inverse_selection_tensor(rec_pose_hands, joint_mask_hands, bs*n)\n",
        "        rec_pose_jaw = rec_pose_jaw.reshape(bs*n, 6)\n",
        "        rec_pose_jaw = rc.rotation_6d_to_matrix(rec_pose_jaw)\n",
        "        rec_pose_jaw = rc.matrix_to_axis_angle(rec_pose_jaw).reshape(bs*n, 1*3)\n",
        "        rec_pose = rec_pose_upper_recover + rec_pose_lower_recover + rec_pose_hands_recover \n",
        "        rec_pose[:, 66:69] = rec_pose_jaw\n",
        "\n",
        "        to_global = rec_lower\n",
        "        to_global[:, :, 54:57] = 0.0\n",
        "        to_global[:, :, :54] = rec_lower2global\n",
        "        rec_global = global_motion(to_global)\n",
        "\n",
        "        rec_trans_v_s = rec_global[\"rec_pose\"][:, :, 54:57]\n",
        "        rec_x_trans = other_tools_hf.velocity2position(rec_trans_v_s[:, :, 0:1], 1/args.pose_fps, tar_trans[:, 0, 0:1])\n",
        "        rec_z_trans = other_tools_hf.velocity2position(rec_trans_v_s[:, :, 2:3], 1/args.pose_fps, tar_trans[:, 0, 2:3])\n",
        "        rec_y_trans = rec_trans_v_s[:,:,1:2]\n",
        "        rec_trans = torch.cat([rec_x_trans, rec_y_trans, rec_z_trans], dim=-1)\n",
        "        tar_pose = tar_pose[:, :n, :]\n",
        "        tar_exps = tar_exps[:, :n, :]\n",
        "        tar_trans = tar_trans[:, :n, :]\n",
        "        tar_beta = tar_beta[:, :n, :]\n",
        "\n",
        "        rec_pose = rc.axis_angle_to_matrix(rec_pose.reshape(bs*n, j, 3))\n",
        "        rec_pose = rc.matrix_to_rotation_6d(rec_pose).reshape(bs, n, j*6)\n",
        "        tar_pose = rc.axis_angle_to_matrix(tar_pose.reshape(bs*n, j, 3))\n",
        "        tar_pose = rc.matrix_to_rotation_6d(tar_pose).reshape(bs, n, j*6)\n",
        "        \n",
        "        net_out =  {\n",
        "            'rec_pose': rec_pose,\n",
        "            'rec_trans': rec_trans,\n",
        "            'tar_pose': tar_pose,\n",
        "            'tar_exps': tar_exps,\n",
        "            'tar_beta': tar_beta,\n",
        "            'tar_trans': tar_trans,\n",
        "            'rec_exps': rec_exps,\n",
        "        }\n",
        "        \n",
        "\n",
        "        tar_pose = net_out['tar_pose']\n",
        "        rec_pose = net_out['rec_pose']\n",
        "        tar_exps = net_out['tar_exps']\n",
        "        tar_beta = net_out['tar_beta']\n",
        "        rec_trans = net_out['rec_trans']\n",
        "        tar_trans = net_out['tar_trans']\n",
        "        rec_exps = net_out['rec_exps']\n",
        "                # print(rec_pose.shape, tar_pose.shape)\n",
        "        bs, n, j = tar_pose.shape[0], tar_pose.shape[1], joints\n",
        "                # interpolate to 30fps  \n",
        "        if (30/args.pose_fps) != 1:\n",
        "            assert 30%args.pose_fps == 0\n",
        "            n *= int(30/args.pose_fps)\n",
        "            tar_pose = torch.nn.functional.interpolate(tar_pose.permute(0, 2, 1), scale_factor=30/args.pose_fps, mode='linear').permute(0,2,1)\n",
        "            rec_pose = torch.nn.functional.interpolate(rec_pose.permute(0, 2, 1), scale_factor=30/args.pose_fps, mode='linear').permute(0,2,1)\n",
        "                \n",
        "                # print(rec_pose.shape, tar_pose.shape)\n",
        "        rec_pose = rc.rotation_6d_to_matrix(rec_pose.reshape(bs*n, j, 6))\n",
        "        rec_pose = rc.matrix_to_axis_angle(rec_pose).reshape(bs*n, j*3)\n",
        "\n",
        "        tar_pose = rc.rotation_6d_to_matrix(tar_pose.reshape(bs*n, j, 6))\n",
        "        tar_pose = rc.matrix_to_axis_angle(tar_pose).reshape(bs*n, j*3)\n",
        "        \n",
        "    return tar_pose, rec_pose, tar_exps, tar_beta, rec_trans, tar_trans, rec_exps, bs, n, j\n",
        "\n",
        "    \n",
        "class BaseTrainer(object):\n",
        "    def __init__(self, args, sp, ap, tp):\n",
        "        hf_dir = \"hf\"\n",
        "        # if not os.path.exists(args.out_path + \"custom/\" + hf_dir + \"/\"):\n",
        "            # os.makedirs(args.out_path + \"custom/\" + hf_dir + \"/\")\n",
        "        # sf.write(args.out_path + \"custom/\" + hf_dir + \"/tmp.wav\", ap[1], ap[0])\n",
        "        # self.audio_path = args.out_path + \"custom/\" + hf_dir + \"/tmp.wav\"\n",
        "        self.audio_path = '/content/EMAGE-hf/EMAGE/test_sequences/wave16k/2_scott_0_1_1.wav'\n",
        "        audio, ssr = librosa.load(self.audio_path)\n",
        "        ap = (ssr, audio)\n",
        "        self.args = args\n",
        "        self.rank = 0 # dist.get_rank()\n",
        "       \n",
        "        #self.checkpoint_path = args.out_path + \"custom/\" + args.name + args.notes + \"/\" #wandb.run.dir #args.cache_path+args.out_path+\"/\"+args.name\n",
        "        self.checkpoint_path = args.out_path + \"custom/\" + hf_dir + \"/\" \n",
        "        if self.rank == 0:\n",
        "            self.test_data = __import__(f\"dataloaders.{args.dataset}\", fromlist=[\"something\"]).CustomDataset(args, \"test\", smplx_path=sp, audio_path=ap, text_path=tp)\n",
        "            self.test_loader = torch.utils.data.DataLoader(\n",
        "                self.test_data, \n",
        "                batch_size=1,  \n",
        "                shuffle=False,  \n",
        "                num_workers=args.loader_workers,\n",
        "                drop_last=False,\n",
        "            )\n",
        "        logger.info(f\"Init test dataloader success\")\n",
        "        model_module = __import__(f\"models.{args.model}\", fromlist=[\"something\"])\n",
        "        \n",
        "        if args.ddp:\n",
        "            self.model = getattr(model_module, args.g_name)(args).to(self.rank)\n",
        "            process_group = torch.distributed.new_group()\n",
        "            self.model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(self.model, process_group)   \n",
        "            self.model = DDP(self.model, device_ids=[self.rank], output_device=self.rank,\n",
        "                             broadcast_buffers=False, find_unused_parameters=False)\n",
        "        else: \n",
        "            self.model = torch.nn.DataParallel(getattr(model_module, args.g_name)(args), args.gpus).cpu()\n",
        "        \n",
        "        if self.rank == 0:\n",
        "            logger.info(self.model)\n",
        "            logger.info(f\"init {args.g_name} success\")\n",
        "\n",
        "        self.smplx = smplx.create(\n",
        "        self.args.data_path_1+\"smplx_models/\", \n",
        "            model_type='smplx',\n",
        "            gender='NEUTRAL_2020', \n",
        "            use_face_contour=False,\n",
        "            num_betas=300,\n",
        "            num_expression_coeffs=100, \n",
        "            ext='npz',\n",
        "            use_pca=False,\n",
        "        )\n",
        "                \n",
        "        self.args = args\n",
        "        self.joints = self.test_data.joints\n",
        "        self.ori_joint_list = joints_list[self.args.ori_joints]\n",
        "        self.tar_joint_list_face = joints_list[\"beat_smplx_face\"]\n",
        "        self.tar_joint_list_upper = joints_list[\"beat_smplx_upper\"]\n",
        "        self.tar_joint_list_hands = joints_list[\"beat_smplx_hands\"]\n",
        "        self.tar_joint_list_lower = joints_list[\"beat_smplx_lower\"]\n",
        "\n",
        "        self.joint_mask_face = np.zeros(len(list(self.ori_joint_list.keys()))*3)\n",
        "        self.joints = 55\n",
        "        for joint_name in self.tar_joint_list_face:\n",
        "            self.joint_mask_face[self.ori_joint_list[joint_name][1] - self.ori_joint_list[joint_name][0]:self.ori_joint_list[joint_name][1]] = 1\n",
        "        self.joint_mask_upper = np.zeros(len(list(self.ori_joint_list.keys()))*3)\n",
        "        for joint_name in self.tar_joint_list_upper:\n",
        "            self.joint_mask_upper[self.ori_joint_list[joint_name][1] - self.ori_joint_list[joint_name][0]:self.ori_joint_list[joint_name][1]] = 1\n",
        "        self.joint_mask_hands = np.zeros(len(list(self.ori_joint_list.keys()))*3)\n",
        "        for joint_name in self.tar_joint_list_hands:\n",
        "            self.joint_mask_hands[self.ori_joint_list[joint_name][1] - self.ori_joint_list[joint_name][0]:self.ori_joint_list[joint_name][1]] = 1\n",
        "        self.joint_mask_lower = np.zeros(len(list(self.ori_joint_list.keys()))*3)\n",
        "        for joint_name in self.tar_joint_list_lower:\n",
        "            self.joint_mask_lower[self.ori_joint_list[joint_name][1] - self.ori_joint_list[joint_name][0]:self.ori_joint_list[joint_name][1]] = 1\n",
        "\n",
        "        self.tracker = other_tools_hf.EpochTracker([\"fid\", \"l1div\", \"bc\", \"rec\", \"trans\", \"vel\", \"transv\", 'dis', 'gen', 'acc', 'transa', 'exp', 'lvd', 'mse', \"cls\", \"rec_face\", \"latent\", \"cls_full\", \"cls_self\", \"cls_word\", \"latent_word\",\"latent_self\"], [False,True,True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False,False,False,False])\n",
        "\n",
        "        vq_model_module = __import__(f\"models.motion_representation\", fromlist=[\"something\"])\n",
        "        self.args.vae_layer = 2\n",
        "        self.args.vae_length = 256\n",
        "        self.args.vae_test_dim = 106\n",
        "        self.vq_model_face = getattr(vq_model_module, \"VQVAEConvZero\")(self.args).cpu()\n",
        "        # print(self.vq_model_face)\n",
        "        # other_tools_hf.load_checkpoints(self.vq_model_face, self.args.data_path_1 + \"pretrained_vq/last_790_face_v2.bin\", args.e_name)\n",
        "        self.args.vae_test_dim = 78\n",
        "        self.vq_model_upper = getattr(vq_model_module, \"VQVAEConvZero\")(self.args).cpu()\n",
        "        # other_tools_hf.load_checkpoints(self.vq_model_upper, self.args.data_path_1 + \"pretrained_vq/upper_vertex_1layer_710.bin\", args.e_name)\n",
        "        self.args.vae_test_dim = 180\n",
        "        self.vq_model_hands = getattr(vq_model_module, \"VQVAEConvZero\")(self.args).cpu()\n",
        "        # other_tools_hf.load_checkpoints(self.vq_model_hands, self.args.data_path_1 + \"pretrained_vq/hands_vertex_1layer_710.bin\", args.e_name)\n",
        "        self.args.vae_test_dim = 61\n",
        "        self.args.vae_layer = 4\n",
        "        self.vq_model_lower = getattr(vq_model_module, \"VQVAEConvZero\")(self.args).cpu()\n",
        "        # other_tools_hf.load_checkpoints(self.vq_model_lower, self.args.data_path_1 + \"pretrained_vq/lower_foot_600.bin\", args.e_name)\n",
        "        self.args.vae_test_dim = 61\n",
        "        self.args.vae_layer = 4\n",
        "        self.global_motion = getattr(vq_model_module, \"VAEConvZero\")(self.args).cpu()\n",
        "        # other_tools_hf.load_checkpoints(self.global_motion, self.args.data_path_1 + \"pretrained_vq/last_1700_foot.bin\", args.e_name)\n",
        "        self.args.vae_test_dim = 330\n",
        "        self.args.vae_layer = 4\n",
        "        self.args.vae_length = 240\n",
        "        \n",
        "        # self.cls_loss = nn.NLLLoss().to(self.rank)\n",
        "        # self.reclatent_loss = nn.MSELoss().to(self.rank)\n",
        "        # self.vel_loss = torch.nn.L1Loss(reduction='mean').to(self.rank)\n",
        "        # self.rec_loss = get_loss_func(\"GeodesicLoss\").to(self.rank) \n",
        "        self.log_softmax = nn.LogSoftmax(dim=2)\n",
        "      \n",
        "    \n",
        "    def inverse_selection(self, filtered_t, selection_array, n):\n",
        "        original_shape_t = np.zeros((n, selection_array.size))\n",
        "        selected_indices = np.where(selection_array == 1)[0]\n",
        "        for i in range(n):\n",
        "            original_shape_t[i, selected_indices] = filtered_t[i]\n",
        "        return original_shape_t\n",
        "    \n",
        "    def inverse_selection_tensor(self, filtered_t, selection_array, n):\n",
        "        selection_array = torch.from_numpy(selection_array).cuda()\n",
        "        original_shape_t = torch.zeros((n, 165)).cuda()\n",
        "        selected_indices = torch.where(selection_array == 1)[0]\n",
        "        for i in range(n):\n",
        "            original_shape_t[i, selected_indices] = filtered_t[i]\n",
        "        return original_shape_t\n",
        "\n",
        "    \n",
        "    def test_demo(self, epoch):\n",
        "        '''\n",
        "        input audio and text, output motion\n",
        "        do not calculate loss and metric\n",
        "        save video\n",
        "        '''\n",
        "        results_save_path = self.checkpoint_path + f\"/{epoch}/\"\n",
        "        if os.path.exists(results_save_path): \n",
        "            import shutil\n",
        "            shutil.rmtree(results_save_path)\n",
        "        os.makedirs(results_save_path)\n",
        "        start_time = time.time()\n",
        "        total_length = 0\n",
        "        test_seq_list = self.test_data.selected_file\n",
        "        align = 0 \n",
        "        latent_out = []\n",
        "        latent_ori = []\n",
        "        l2_all = 0 \n",
        "        lvel = 0\n",
        "        for its, batch_data in enumerate(self.test_loader):\n",
        "            tar_pose, rec_pose, tar_exps, tar_beta, rec_trans, tar_trans, rec_exps, bs, n, j = test_demo_gpu(\n",
        "                self.model, self.vq_model_face, self.vq_model_upper, self.vq_model_hands, self.vq_model_lower, self.global_motion, self.smplx,\n",
        "                batch_data,\n",
        "                self.args,\n",
        "                self.joints, self.joint_mask_upper, self.joint_mask_lower, self.joint_mask_hands,\n",
        "                self.log_softmax,\n",
        "            )\n",
        "                           \n",
        "            tar_pose_np = tar_pose.detach().cpu().numpy()\n",
        "            rec_pose_np = rec_pose.detach().cpu().numpy()\n",
        "            rec_trans_np = rec_trans.detach().cpu().numpy().reshape(bs*n, 3)\n",
        "            rec_exp_np = rec_exps.detach().cpu().numpy().reshape(bs*n, 100)\n",
        "            tar_exp_np = tar_exps.detach().cpu().numpy().reshape(bs*n, 100)\n",
        "            tar_trans_np = tar_trans.detach().cpu().numpy().reshape(bs*n, 3)\n",
        "                #'''\n",
        "        # its = 0\n",
        "            gt_npz = np.load(self.args.data_path+self.args.pose_rep +\"/\"+test_seq_list.iloc[its]['id']+\".npz\", allow_pickle=True)\n",
        "            np.savez(results_save_path+\"gt_\"+test_seq_list.iloc[its]['id']+'.npz',\n",
        "                    betas=gt_npz[\"betas\"],\n",
        "                    poses=tar_pose_np,\n",
        "                    expressions=tar_exp_np,\n",
        "                    trans=tar_trans_np,\n",
        "                    model='smplx2020',\n",
        "                    gender='neutral',\n",
        "                    mocap_frame_rate = 30,\n",
        "                )\n",
        "            np.savez(results_save_path+\"res_\"+test_seq_list.iloc[its]['id']+'.npz',\n",
        "                    betas=gt_npz[\"betas\"],\n",
        "                    poses=rec_pose_np,\n",
        "                    expressions=rec_exp_np,\n",
        "                    trans=rec_trans_np,\n",
        "                    model='smplx2020',\n",
        "                    gender='neutral',\n",
        "                    mocap_frame_rate = 30,\n",
        "                )\n",
        "        \n",
        "            total_length += n\n",
        "            # render_vid_path = other_tools_hf.render_one_sequence_no_gt(\n",
        "            # results_save_path+\"res_\"+test_seq_list.iloc[its]['id']+'.npz', \n",
        "            #         # results_save_path+\"gt_\"+test_seq_list.iloc[its]['id']+'.npz', \n",
        "            #         results_save_path,\n",
        "            #         self.audio_path,\n",
        "            #         self.args.data_path_1+\"smplx_models/\",\n",
        "            #         use_matplotlib = False,\n",
        "            #         args = self.args,\n",
        "            #         )\n",
        "            render_vid_path = other_tools_hf.render_one_sequence_with_face(\n",
        "                    results_save_path+\"res_\"+test_seq_list.iloc[its]['id']+'.npz',\n",
        "                    results_save_path+\"gt_\"+test_seq_list.iloc[its]['id']+'.npz',\n",
        "                    results_save_path,\n",
        "                    self.audio_path,\n",
        "                    self.args.data_path_1+\"smplx_models/\",\n",
        "                    use_matplotlib = False,\n",
        "                    args = self.args,\n",
        "                    )\n",
        "        # result = gr.Video(value=render_vid_path, visible=True)\n",
        "        # end_time = time.time() - start_time\n",
        "        # logger.info(f\"total inference time: {int(end_time)} s for {int(total_length/self.args.pose_fps)} s motion\")\n",
        "        return render_vid_path\n",
        "\n",
        "@logger.catch\n",
        "def emage(audio_path):\n",
        "    smplx_path = None\n",
        "    text_path = None\n",
        "    rank = 0\n",
        "    world_size = 1\n",
        "\n",
        "    # args = config.parse_args()\n",
        "    args = Options()\n",
        "    idc = 0\n",
        "    for i, char in enumerate(args.config):\n",
        "      if char == \"/\": idc = i\n",
        "    args.name = args.config[idc+1:-5]\n",
        "    is_train = args.is_train\n",
        "    if is_train:\n",
        "      time_local = time.localtime()\n",
        "      name_expend = \"%02d%02d_%02d%02d%02d_\"%(time_local[1], time_local[2],time_local[3], time_local[4], time_local[5])\n",
        "      args.name = name_expend + args.name\n",
        "\n",
        "    #os.environ['TRANSFORMERS_CACHE'] = args.data_path_1 + \"hub/\"\n",
        "    if not sys.warnoptions:\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "    # dist.init_process_group(backend=\"gloo\", rank=rank, world_size=world_size)\n",
        "\n",
        "    #logger_tools.set_args_and_logger(args, rank)\n",
        "    other_tools_hf.set_random_seed(args)\n",
        "    other_tools_hf.print_exp_info(args)\n",
        "\n",
        "    # return one intance of trainer\n",
        "    print(audio_path)\n",
        "    trainer = BaseTrainer(args, sp = smplx_path, ap = audio_path, tp = text_path)\n",
        "    result = trainer.test_demo(999)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"PYOPENGL_PLATFORM\"] = \"egl\"\n",
        "os.environ[\"MESA_GL_VERSION_OVERRIDE\"] = \"4.1\"\n",
        "result = emage('/content/EMAGE-hf/EMAGE/test_sequences/wave16k/2_scott_0_1_1.wav')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
